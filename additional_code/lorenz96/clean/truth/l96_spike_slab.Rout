
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> set.seed(1)
> library(mvtnorm)
> library(mcmc)
> library(invgamma)
> 
> make_tilde <- function(X, t) {
+     X_vec = c(1, X[1], X[2], X[3], X[4], X[1] ^ 2, X[2] ^ 2, X[3] ^ 2, X[4] ^ 2, X[1] * X[2], X[1] * X[3], X[1] * X[4], 
+               X[2] * X[3], X[2] * X[4], X[3]*X[4], t, t ^ 2)
+     return(X_vec)
+ }
> # drifet function for Lorenz-63
> drift_fun <- function(X, t, B) {
+     #print(make_tilde(X,t))
+     tildeX = matrix(make_tilde(X, t), nrow = 17, ncol = 1)
+     B_mat = matrix(B, nrow = N.l96)
+     #print(B)
+     #print(dim(tildeX))
+     ans = B_mat %*% tildeX
+     return(ans)
+ }
> 
> drift_fun_true <- function(X, theta) {
+     ans = matrix(, nrow = N.l96, ncol = 1)
+     for (i in 0:(N.l96-1)) {
+         ans[i + 1, 1] = (X[(i + 1) %% N.l96 + 1] - X[(i - 2) %% N.l96 + 1]) * X[(i - 1) %% N.l96 + 1] - X[i + 1] + theta
+     }
+     return(ans)
+ }
> 
> ludfun <- function(state, gamma) {
+     # State is the vector storing the vectors of length 3*N + 12. The first 3*(N+1) terms are Xs. The next three terms are the parameters \sigma, \rho & 
+     # \beta. The remaining 6 terms are the \Sigma matrix. Definition of Sigma below shows how the symmetric matrix is constructed.
+ 
+     X_n = matrix(state[1:n.X], nrow = N.l96, ncol = N + 1)
+     B_vec = state[(n.X + 1):(n.X + n.theta)] # vector of \sigma, \rho and \beta    
+     B_mat = matrix(B_vec, nrow = N.l96)
+     # all the elements of theta should be positive
+     #if (min(theta) <= 0)
+     #return(-Inf)
+ 
+     # Extracting observed data
+     X_t = X_n[, seq(2, N + 1, N / K)]
+ 
+ 
+     # pi is the log of likelihood
+     # This doesn't need a loop
+     #p1 = 0
+     ##print(dim(Y))
+     #for (k in 1:K) {
+     #Y.t = t(t(Y[, k]))
+     #X_t.t = t(t(X_t[, k]))
+     #p1 = p1 + t(Y.t - X_t.t) %*% inv_R %*% (Y.t - X_t.t)
+     #}
+     #p1 = -0.5 * p1
+     #p1 = p1 - 0.5 * t(t(t(X_n[, 1])) - tau_o) %*% inv.lam_o %*% (t(t(X_n[, 1])) - tau_o)
+ 
+     #######################################################################
+     p1 = (sum(dmvnorm(t(Y - X_t), sigma = R, log = TRUE)))
+     #- 0.5 * t(t(t(X_n[, 1])) - tau_o) %*% inv.lam_o %*% (t(t(X_n[, 1])) - tau_o))
+     ######################################################################
+     B_cov_gamma = gamma * (tau1 ^ 2) + (1 - gamma) * (tau0 ^ 2)
+     p2 = dmvnorm(B_vec, sigma = diag(B_cov_gamma), log = TRUE)
+     #p2 = (alpha1 - 1) * log(theta[1]) - theta[1] / beta1 + (alpha2 - 1) * log(theta[2]) - theta[2] / beta2 + (alpha3 - 1) * log(theta[3]) - theta[3] / beta3
+ 
+ 
+     f = mapply(drift_fun, X = split(X_n, rep(1:ncol(X_n), each = nrow(X_n))), t = del_t * (0:N), MoreArgs = list(B_vec))
+     #f = sapply(split(X_n, rep(1:ncol(X_n), each = nrow(X_n))), drift_fun, B_vec, list(1,2))
+     del_X = t(diff(t(X_n)))
+     beta_tmp = rowSums((del_X / del_t - f[, - (N + 1)]) ^ 2) * del_t / 2
+     p3 = -(a4 + N / 2) * sum(log(b4 + beta_tmp))
+ 
+     return(p1 + p2 + p3)
+ 
+ }
> 
> ludfun.X <- function(state, gamma, all) {
+     # State is the vector storing the vectors of length 3*N + 12. The first 3*(N+1) terms are Xs. The next three terms are the parameters \sigma, \rho & 
+     # \beta. The remaining 6 terms are the \Sigma matrix. Definition of Sigma below shows how the symmetric matrix is constructed.
+     #if (index == 0) {
+     ##print('0')
+     #all[1:n.X] = state
+     #} else {
+     ##print(index)
+     #all[n.X+index] = state
+     #}
+     all[1:n.X] = state
+     X_n = matrix(all[1:n.X], nrow = N.l96, ncol = N + 1)
+     B_vec = all[(n.X + 1):(n.X + n.theta)] # vector of \sigma, \rho and \beta    
+     B_mat = matrix(B_vec, nrow = N.l96)
+ 
+     #X_n = matrix(state[1:n.X], nrow = 3, ncol = N + 1)
+     #B_vec = state[(n.X + 1):(n.X + n.theta)] # vector of \sigma, \rho and \beta    
+     #B_mat = matrix(B_vec, nrow = 3)
+ 
+     # all the elements of theta should be positive
+     #if (min(theta) <= 0)
+     #return(-Inf)
+ 
+     # Extracting observed data
+     X_t = X_n[,seq(2, N + 1, N / K)]
+ 
+ 
+     # pi is the log of likelihood
+     # This doesn't need a loop
+     #p1 = 0
+     ##print(dim(Y))
+     #for (k in 1:K) {
+     #Y.t = t(t(Y[, k]))
+     #X_t.t = t(t(X_t[, k]))
+     #p1 = p1 + t(Y.t - X_t.t) %*% inv_R %*% (Y.t - X_t.t)
+     #}
+     #p1 = -0.5 * p1
+     #p1 = p1 - 0.5 * t(t(t(X_n[, 1])) - tau_o) %*% inv.lam_o %*% (t(t(X_n[, 1])) - tau_o)
+ 
+     #######################################################################
+     p1 = (sum(dmvnorm(t(Y - X_t), sigma = R, log = TRUE)))
+     #- 0.5 * t(t(t(X_n[, 1])) - tau_o) %*% inv.lam_o %*% (t(t(X_n[, 1])) - tau_o))
+     ######################################################################
+     B_cov_gamma = gamma * (tau1 ^ 2) + (1 - gamma) * (tau0 ^ 2)
+     p2 = dmvnorm(B_vec, sigma = diag(B_cov_gamma), log = TRUE)
+     #p2 = (-1 / 2) * sum((B_vec - mu) ^ 2) / sigma2
+ 
+     #f = mapply(drift_fun, X = split(X_n, rep(1:ncol(X_n), each = nrow(X_n))), t = del_t * (0:N), MoreArgs = list(B_vec))
+     f = mapply(drift_fun, X = split(X_n, rep(1:ncol(X_n), each = nrow(X_n))), t = del_t * (0:N), MoreArgs = list(B_vec))
+     #f = sapply(split(X_n, rep(1:ncol(X_n), each = nrow(X_n))), drift_fun, B_vec, list(1,2))
+     del_X = t(diff(t(X_n)))
+     beta_tmp = rowSums((del_X / del_t - f[, - (N + 1)]) ^ 2) * del_t / 2
+     p3 = -(a4 + N / 2) * sum(log(b4 + beta_tmp))
+ 
+     return(p1 + p2 + p3)
+ 
+ }
> 
> sample_gamma <- function(B_vec) {
+     gamma = numeric(length = n.theta)
+     for (i in 1:n.theta) {
+         prob = q[i] * dnorm(B_vec[i], sd = tau1) / (q[i] * dnorm(B_vec[i], sd = tau1) + (1 - q[i]) * dnorm(B_vec[i], sd = tau0))
+         gamma[i] = rbinom(1, 1, prob)
+     }
+     return(gamma)
+ }
> 
> MH.X <- function(init, n, scale, gamma, B_vec) {
+     chain = matrix(, nrow = n, ncol = n.X)
+     accept.prob = 0
+     for (i in 1:n) {
+         prop = sapply(init, function(t) rnorm(1, t, scale))
+         prop_ludf = c(prop, B_vec)
+         init_ludf = c(init, B_vec)
+         if (log(runif(1)) < (ludfun(prop_ludf, gamma) - ludfun(init_ludf, gamma))) {
+             init = prop
+             accept.prob = accept.prob + 1
+         }
+         chain[i,] = init
+     }
+     ans = list(chain, accept.prob / n)
+     return(ans)
+ }
> 
> MH.B <- function(index, init, n, scale, gamma, state) {
+     chain = numeric(length = n)
+     accept.prob = 0
+     prop_ludf = state
+     init_ludf = state
+     for (i in 1:n) {
+         prop = rnorm(1, init, scale)
+         prop_ludf[n.X + index] = prop
+         init_ludf[n.X + index] = init
+         if (log(runif(1)) < (ludfun(prop_ludf, gamma) - ludfun(init_ludf, gamma))) {
+             init = prop
+             accept.prob = accept.prob + 1
+         }
+         chain[i] = init
+     }
+     ans = list(chain, accept.prob / n)
+     return(ans)
+ }
> 
> linchpin <- function(n, init, scale_vec) {
+     X_avg = numeric(length = n.X)
+     param_mat = matrix(, nrow = n, ncol = 2 * n.theta + n.sigma)
+     scale = rep(0.0001 * 1, n.X + n.theta)
+ 
+     scale[(n.X + 1):(n.X + n.theta)] = scale_vec # tf = 5 Sigma = 0.1  change of above
+     #scale[n.X + c(1)] = .5 * scale_vec[c(1)]   ## Sigma = 5
+     #scale[n.X + c(2)] = 2.5 * scale_vec[c(2)]
+     #scale[n.X + c(3)] = 15 * scale_vec[c(3)]
+     #scale[n.X + c(4)] = 5 * scale_vec[c(4)]
+     #scale[n.X + c(5)] = 0.5 * scale_vec[c(5)]
+     #scale[n.X + c(6, 7, 9, 13, 14, 16,61, 62, 63, 64)] = 2 * scale_vec[c(6, 7, 9, 13, 14, 16, 61, 62, 63, 64)]
+     #scale[n.X + c(11, 21, 25, 26, 32, 35, 39, 68)] = 0.6 * scale_vec[c(11, 21, 25, 26, 32, 35, 39, 68)]
+     #scale[n.X + c(8)] = 0.3 * scale_vec[c(8)]
+     #scale[n.X + c(10, 24, 63, 67)] = 0.4 * scale_vec[c(10, 24, 63, 67)]
+     #scale[n.X + c(12)] = 0.5 * scale_vec[c(12)]
+     #scale[n.X + c(17, 18, 19,20)] = 1.8 * scale_vec[c(17, 18, 19,20)]
+ 
+ 
+     scale[n.X + c(1)] = .4 * scale_vec[c(1)]   ## Sigma = .5
+     scale[n.X + c(2)] = 2.5 * scale_vec[c(2)]
+     scale[n.X + c(3)] = 12 * scale_vec[c(3)]
+     scale[n.X + c(4)] = 5 * scale_vec[c(4)]
+     scale[n.X + c(5,25,28)] = 0.5 * scale_vec[c(5,25,28)]
+     scale[n.X + c(9)] = 2.5 * scale_vec[c(9)]
+     scale[n.X + c(6, 9, 16)] = 3 * scale_vec[c(6, 9, 16)]
+     scale[n.X + c(8,29,32)] = 0.3 * scale_vec[c(8,29,32)]
+     scale[n.X + c(10, 26,31,67)] = 0.3 * scale_vec[c(10, 26,31,67)]
+     scale[n.X + c(11, 12,21, 24,27,30,34,35,39,52,55,65,68)] = 0.6 * scale_vec[c(11,12, 21, 24,27,30, 34,35,39,52,55,65,68)]
+     scale[n.X + c(17, 19)] = 2.5 * scale_vec[c(17, 19)]
+     scale[n.X + c(7,13,14,18, 61, 62, 64)] = 2 * scale_vec[c(7,13,14,18, 61, 62, 64)]
+     scale[n.X + c(20)] = 1.8 * scale_vec[c(20)]
+     scale[n.X + c(23)] = 1.5 * scale_vec[c(23)]
+     scale[n.X + c(59)] = 1.2 * scale_vec[c(59)]
+     scale[n.X + c(15,63)] = 0.8 * scale_vec[c(15,63)]
+     
+ 
+     scale.X = 0.0015 #0.0025 #####################################tried decreasing this?
+     scale.B = scale[(n.X + 1):(n.X + n.theta)]
+ 
+     accept.prob = numeric(1 + n.theta)
+     state = init
+ 
+     for (i in 1:n) {
+         gamma = sample_gamma(init[(n.X + 1):(n.X + n.theta)])
+         param_mat[i, (n.theta + n.sigma + 1):(2 * n.theta + n.sigma)] = gamma
+ 
+         if (i %% (n / 5) == 0) {
+             print(i)
+             print(matrix(accept.prob[2:(n.theta + 1)] / i, nrow = N.l96))
+         }
+ 
+         all = init
+         chain = metrop(ludfun.X, initial = init[1:n.X], nbatch = 1, scale = scale.X, gamma = gamma, all = all)
+         accept.prob[1] = accept.prob[1] + chain$accept
+         state[1:n.X] = chain$batch
+ 
+         #ans = MH.X(init[1:n.X], 1, scale.X, gamma, init[(n.X + 1):(n.X + n.theta)])
+         #accept.prob[1] = accept.prob[1] + ans[[2]]
+         #init[1:n.X] = ans[[1]]
+ 
+         for (j in 1:n.theta) {
+             #all = init
+             #chain = metrop(ludfun, initial = init[n.X + j], nbatch = 1, scale = scale.B[j], gamma = gamma, all = all, index = j)
+             #accept.prob[j + 1] = accept.prob[j + 1] + chain$accept
+             #init[n.X + j] = chain$batch
+             # state = init  ################### this was not there in previous code
+             ans = MH.B(j, init[n.X + j], 1, scale.B[j], gamma, state)
+             accept.prob[j + 1] = accept.prob[j + 1] + ans[[2]]
+             state[n.X + j] = ans[[1]]
+         }
+         #state = init
+         #chain = metrop(ludfun, init, 1, scale = scale, gamma = gamma)
+         #state = chain$batch
+         #accept.prob = accept.prob + chain$accept
+ 
+         X_n = matrix(state[1:n.X], nrow = N.l96, ncol = N + 1)
+         theta = state[(n.X + 1):(n.X + n.theta)] # vector of \sigma, \rho and \beta 
+         X_avg = X_avg + state[1:n.X]
+         param_mat[i, 1:n.theta] = theta
+ 
+         Sigma = numeric(length = n.sigma)
+         f = mapply(drift_fun, X = split(X_n, rep(1:ncol(X_n), each = nrow(X_n))), t = del_t * (0:N), MoreArgs = list(theta))
+         del_X = t(diff(t(X_n)))
+         beta_tmp = rowSums((del_X / del_t - f[, - (N + 1)]) ^ 2) * del_t / 2
+         for (j in 1:n.sigma) {
+             Sigma[j] = rinvgamma(1, shape = N / 2 + a4, rate = b4 + beta_tmp[j])
+         }
+ 
+         param_mat[i, (n.theta + 1):(n.theta + n.sigma)] = Sigma
+         init = state
+     }
+     print(accept.prob / n)
+     X_avg = X_avg / n
+     final_output = list(param_mat, X_avg, accept.prob / n)
+     return(final_output)
+ }
> 
> 
> # Numerical method to sample from SDE
> euler_maruyama <- function(X0, del_t, N, theta, Sigma) {
+     X = matrix(, nrow = N.l96, ncol = N + 1)
+     X[, 1] = X0
+     for (i in 2:(N + 1))
+         X[, i] = X[, i - 1] + t(drift_fun_true(X[, i - 1], theta)) * del_t + rmvnorm(1, sigma = del_t * Sigma)
+     return(X)
+ }
> # X = euler_maruyama(c(1,1,1), 0.1, 20, c(1,2,3), diag(2,3))
> 
> 
> # hyper-parameters
> to = 0 # initial time
> tf = 10 # final time
> Nobs = 20 # no of observations (Y) per time step
> N.l96 = 4
> del_t = 0.01 # discrete approximation of dt
> a4 = 2
> b4 = .5
> tau1 = 10
> tau0 = 0.5
> 
> K = (tf - to) * Nobs # no of real life observations, i.e. size of Y
> N = (tf - to) / del_t # no of discretizations of the Lorenz-63, i.e. size of X
> seq.Y = seq(2, N + 1, N / K)
> N = tail(seq.Y, 1)
> burn_in = 5000 #/ del_t
> R = diag(.05, N.l96) # observational error
> inv_R = diag(1 / (0.05), N.l96)
> mu = 0
> sigma2 = 10
> mu_truth = c(rep(8, 4), as.numeric(diag(rep(-1, 4))), rep(0,16),0,0,-1,0,0,1,0,1,0,-1, rep(0,5), -1,1,0,1,0,-1,rep(0,11)) #c(-10, 28, 0, 10, -1, rep(0, 3), -8 / 3, rep(0, 11), 1, rep(0, 4), -1, rep(0, 7))
> non_zero = c(1,2,3,4,5,10,15,20,39,42,44,46,52,53,55,57)
> param_i = 1:4
> n.X = N.l96 * (N + 1)
> n.theta = 68
> n.sigma = N.l96
> n.param = n.X + n.theta + n.sigma
> q = rep(0.1, n.theta) #runif(n.theta)
> q[non_zero] = 0.9
> n <- 5e4    
> 
> X_total = euler_maruyama(rep(0, N.l96), del_t, N + burn_in, 8, diag(.5, N.l96)) # generating sample from Lorenz-63
> X = X_total[, (burn_in):(N + burn_in)]
> X = X[, 1:(N + 1)]
> Y = X[,seq(2, N + 1, N / K)] + rnorm(K, sd = sqrt(R)) # observations from Lorenz-63
> init = numeric(n.X + n.theta)
> 
> # STARTING FROM THE TRUTH
> #init[(1:n.X)] <- as.numeric(X) #runif(n.param, 0, 5)
> #init[(n.X + 1):(n.X + n.theta)] = mu_truth
> 
> init[(1:n.X)] <- as.numeric(X) #+ rnorm(n.X) #runif(n.param, 0, 5)
> init[(n.X + 1):(n.X + n.theta)] <- rmvnorm(1, mu_truth , sigma = diag(1 / 50, n.theta)) #rmvnorm(1, c(10, 28, 8 / 3), sigma = diag(0.5, 3)) # random initial values for MCMC
> #load('l96_5e3_cwise_spikes_truth_diffuse')
> #ans = to_save[[1]]
> #pm = ans[[1]][, 1:(n.sigma + n.theta)]
> #init[(n.X + 1):(n.X + n.theta)] = colMeans(pm[3e3:5e3, 1:n.theta])
> #init[1:n.X] = ans[[2]]
> 
> sigma_Y = mean(diag(var(t(Y))))
> tau0 = sqrt(sigma_Y / (10 * K)) * 1.5
> tau1 = sqrt(sigma_Y * max((n.theta ^ 2.1) / (100 * K), log(K))) / 2
> 
> load('l96_linch_spike_5e2')
> var1 = cov(to_save[[1]][[1]][, 1:n.theta])
> scale_vec = 1.6*sqrt(diag(var1))
> #ans2 = linchpin(n, init, rep(.2,n.theta))
> 
> ans = linchpin(n, init, scale_vec)
[1] 10000
       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]
[1,] 0.2192 0.2385 0.1840 0.3001 0.2457 0.2089 0.2340 0.3076 0.2033 0.2671
[2,] 0.2530 0.2436 0.2236 0.2665 0.2326 0.2436 0.3310 0.2333 0.2840 0.2308
[3,] 0.2308 0.2355 0.2709 0.2279 0.2417 0.2449 0.2433 0.3632 0.2596 0.2857
[4,] 0.2292 0.3112 0.1890 0.2244 0.2379 0.2276 0.2637 0.3249 0.2072 0.3146
      [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]
[1,] 0.3126 0.3196 0.2278 0.1928 0.2110 0.2320 0.2469
[2,] 0.2675 0.2562 0.2563 0.2591 0.2758 0.2323 0.1762
[3,] 0.2881 0.3269 0.2390 0.2794 0.2313 0.2089 0.2396
[4,] 0.2316 0.2879 0.2536 0.2942 0.2967 0.2300 0.2738
[1] 20000
        [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]
[1,] 0.21480 0.23590 0.17970 0.29820 0.23820 0.20335 0.22950 0.30530 0.19590
[2,] 0.25020 0.24365 0.21905 0.25875 0.22775 0.23495 0.32770 0.23225 0.27565
[3,] 0.22645 0.23620 0.27290 0.22555 0.23925 0.24255 0.23990 0.35570 0.26085
[4,] 0.23320 0.31635 0.19020 0.22675 0.24585 0.22825 0.27185 0.32705 0.21265
       [,10]   [,11]   [,12]   [,13]   [,14]   [,15]   [,16]   [,17]
[1,] 0.26325 0.30425 0.31010 0.22025 0.18735 0.20115 0.22500 0.23735
[2,] 0.23025 0.26140 0.25010 0.24890 0.25745 0.26735 0.22670 0.17025
[3,] 0.28590 0.28690 0.32985 0.24240 0.28085 0.22895 0.21210 0.24110
[4,] 0.31950 0.23965 0.28850 0.25680 0.29890 0.29905 0.23715 0.28110
[1] 30000
          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
[1,] 0.2119667 0.2338000 0.1812667 0.2959000 0.2374667 0.2010000 0.2290333
[2,] 0.2506667 0.2440000 0.2220000 0.2632000 0.2270667 0.2348000 0.3295667
[3,] 0.2255667 0.2352667 0.2715667 0.2227000 0.2394667 0.2433667 0.2400333
[4,] 0.2344667 0.3166667 0.1917667 0.2276667 0.2466000 0.2280333 0.2727333
          [,8]      [,9]     [,10]     [,11]  [,12]     [,13]     [,14]
[1,] 0.2996667 0.1944333 0.2631667 0.3045000 0.3075 0.2211000 0.1853000
[2,] 0.2306000 0.2786333 0.2314000 0.2612000 0.2534 0.2514667 0.2609333
[3,] 0.3535333 0.2637333 0.2887667 0.2862333 0.3303 0.2420000 0.2756667
[4,] 0.3287000 0.2172667 0.3223000 0.2397667 0.2905 0.2551333 0.3012000
         [,15]     [,16]     [,17]
[1,] 0.2017000 0.2280333 0.2365667
[2,] 0.2673000 0.2283667 0.1710333
[3,] 0.2329000 0.2121000 0.2411000
[4,] 0.3021333 0.2380000 0.2787333
[1] 40000
         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
[1,] 0.208950 0.230350 0.180850 0.292275 0.236400 0.199900 0.228875 0.299100
[2,] 0.250350 0.244625 0.223100 0.262600 0.227950 0.235225 0.329725 0.230400
[3,] 0.223575 0.235000 0.268575 0.219650 0.239375 0.240550 0.238750 0.352975
[4,] 0.235225 0.319200 0.194125 0.228425 0.247150 0.230450 0.275200 0.329025
         [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]
[1,] 0.193200 0.263600 0.303000 0.306775 0.221000 0.187475 0.201525 0.226525
[2,] 0.280625 0.230725 0.260575 0.255350 0.251950 0.262600 0.266650 0.228675
[3,] 0.262725 0.287875 0.287350 0.327125 0.241375 0.272725 0.230900 0.209625
[4,] 0.217725 0.323000 0.242225 0.292250 0.256775 0.302725 0.301250 0.239150
        [,17]
[1,] 0.233875
[2,] 0.171175
[3,] 0.238850
[4,] 0.281200
[1] 50000
        [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]
[1,] 0.20816 0.22974 0.17900 0.28984 0.23396 0.19980 0.22762 0.29662 0.19256
[2,] 0.25096 0.24608 0.22548 0.26510 0.22764 0.23714 0.33158 0.23116 0.28148
[3,] 0.22106 0.23366 0.26746 0.21808 0.23748 0.23954 0.23722 0.35032 0.25948
[4,] 0.23628 0.32158 0.19432 0.22812 0.24830 0.23056 0.27648 0.32942 0.21894
       [,10]   [,11]   [,12]   [,13]   [,14]   [,15]   [,16]   [,17]
[1,] 0.26014 0.30134 0.30534 0.21996 0.18574 0.20014 0.22450 0.23146
[2,] 0.23238 0.26048 0.25626 0.25432 0.26458 0.26878 0.22892 0.17198
[3,] 0.28550 0.28772 0.32458 0.23942 0.27000 0.22968 0.20778 0.23634
[4,] 0.32228 0.24338 0.29260 0.25572 0.30488 0.30160 0.24080 0.28362
 [1] 0.32092 0.20816 0.25096 0.22106 0.23628 0.22974 0.24610 0.23368 0.32158
[10] 0.17900 0.22550 0.26746 0.19434 0.28984 0.26512 0.21810 0.22812 0.23396
[19] 0.22764 0.23748 0.24830 0.19980 0.23714 0.23954 0.23056 0.22762 0.33158
[28] 0.23724 0.27648 0.29664 0.23116 0.35034 0.32944 0.19256 0.28148 0.25948
[37] 0.21894 0.26014 0.23238 0.28550 0.32228 0.30134 0.26050 0.28774 0.24338
[46] 0.30534 0.25626 0.32458 0.29260 0.21996 0.25434 0.23942 0.25572 0.18574
[55] 0.26458 0.27000 0.30488 0.20014 0.26878 0.22968 0.30162 0.22452 0.22892
[64] 0.20778 0.24080 0.23146 0.17198 0.23634 0.28362
> #plot.ts(ans[[1]][, param_i])
> #plot.ts(ans[[1]][, non_zero])
> chain_info = capture.output(cat("no of samples from MC is ", n, " \n starting from previous run ", "\n priors spike slab ", " time period ",
+                                 tf, " Sigma is 1"))
> 
> print(chain_info)
[1] "no of samples from MC is  50000  "               
[2] " starting from previous run  "                   
[3] " priors spike slab   time period  10  Sigma is 1"
> to_save = list(ans, chain_info)
> save(to_save, file = "l96_5e4_cwise_spikes_truth_diffuse")
> pm = ans[[1]][, 1:(n.sigma + n.theta)]
> 
> print(matrix(colMeans(pm), nrow = N.l96))
         [,1]        [,2]         [,3]        [,4]        [,5]         [,6]
[1,] 8.772625 -0.70005362  0.037495681  0.02721653 -0.05022680 -0.030722404
[2,] 7.152431  0.04167617 -0.880600276 -0.04716383 -0.06651628  0.002144229
[3,] 7.581368  0.02779374  0.019040874 -0.49967546  0.02320907 -0.005143361
[4,] 6.683803  0.05774388  0.004887014 -0.01950393 -1.17173054  0.006132836
             [,7]         [,8]         [,9]        [,10]       [,11]
[1,] -0.015805332 -0.002102401  0.005342076 -0.003627503 -0.03853318
[2,] -0.007740897  0.017154953 -0.004203197 -0.024123439  1.01231844
[3,] -0.006580198 -0.058478470 -0.001415967 -0.992835452 -0.05535566
[4,]  0.026807920  0.029413096  0.049295917 -0.060592596  1.00024464
            [,12]       [,13]       [,14]       [,15]        [,16]        [,17]
[1,]  0.002611136 -0.01997958  0.96929711 -0.99002257 -0.036489532 -0.005079884
[2,] -0.996644673  0.02453581 -0.03815286  0.02795697 -0.014306783  0.011010609
[3,]  0.027935955  0.03296472  0.96466724 -0.01497073 -0.075316227  0.018381629
[4,] -0.011945653 -1.01854494  0.06041434 -0.03849089 -0.001963712 -0.003140899
         [,18]
[1,] 0.3840905
[2,] 0.4171835
[3,] 0.5094269
[4,] 0.5896943
> 
> pm2 = ans[[1]][, (n.sigma + n.theta + 1):(n.sigma + 2 * n.theta)]
> print(matrix(colMeans(pm2), nrow = N.l96))
     [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]
[1,]    1 0.94084 0.01128 0.01286 0.00726 0.00320 0.00334 0.00328 0.00310
[2,]    1 0.01562 0.97730 0.01320 0.00908 0.00360 0.00312 0.00346 0.00300
[3,]    1 0.00794 0.00826 0.78192 0.00714 0.00354 0.00354 0.00402 0.00300
[4,]    1 0.01678 0.00680 0.00908 1.00000 0.00344 0.00314 0.00304 0.00408
       [,10]   [,11]   [,12]   [,13]   [,14]   [,15]   [,16]   [,17]
[1,] 0.00326 0.00366 0.00328 0.00270 1.00000 1.00000 0.00868 0.00368
[2,] 0.00348 1.00000 1.00000 0.00358 0.00334 0.00316 0.01594 0.00356
[3,] 1.00000 0.00406 0.00346 0.00350 1.00000 0.00322 0.02704 0.00334
[4,] 0.00354 1.00000 0.00338 1.00000 0.00366 0.00312 0.00962 0.00354
> 
> proc.time()
     user    system   elapsed 
69525.265   232.249 73543.049 
